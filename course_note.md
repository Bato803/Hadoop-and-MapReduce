# Hapdoop and MapReduce Notes

## Core Hadoop
1. Store data in HDFS (Hapdoop distributed file system), and process data in MapReduce. 
2. Key of the Core Hadoop system work: 
  - We split the data, and store it across a collection of machines, known as a cluster. And we process it where it was stored, rather than retrieving data from central server. So it's processed in place. 
  - More machine can be added to the cluster as the amount of data grows. 
3. Hadoop ecosystem:
  - Hive(turns SQL into MapReduce code, and then run it in the cluster), Pig; (These two might need more time because it's still running MapReduce code.)
  - Impala: Write SQL to directly access HDFS, it's optimized for low latency query. (No need to translated to MapReduce.) Hive is optimzied for long, batched processing job. 
  - Sqoop: Takes data from traditional relational database, and put it in HDFS files. 
  - Flume: Put external generated data into HDFS. 
  - HBase/Hue/Oozie/mahout....
  - CDH: Package all of them...Far easier to install CDH to have everything. 
  

## Hadoop Distributed File System
1. One file is split into different blocks, such as blk_1, blk_2, blk_3. 
2. Each block is store in one node of the cluster, separately. Namenode store the metadata about which block is stored in which node. 
3. To make the system more safe, Hadoop doesn't just store the block in one node. But in three nodes instead. This takes care of the problem that if one of the data nodes fail. 
4. To solve the problem if the meta node fails, people doesn't just store the namenode in its own harddrive, but also somewhere in the network system (NFS); There is an alternative solution as well, active namenode and standby namenode. 
5. Mapper and Reducer. 
  - Each mapper deals with a small amount of data and works in parallel, the output is called intermedimate records. (The record is dealt with in the form of (key, value))
  - Once the mappers have finished, a phase called shuffle and sort take place. Shuffle: the movement of intermediate record from mappers to reducers; Sort: the reducer would organize the records. 
6. When we run a map reduce job, we submit the job to what's called job tracker. That splits the job into mappers and reducers. 
  - There is a task tracker in each of the data node. The task tracker and the data node run at the same machine, the Hadoop framework would be able to have the map task tracker work directly on the pieces of data that are stored on the machine. That save a lot of traffics. 
  - Each mapper processes a portion of the input data, that's known as the input split. And by default, Hadoop would use a HDFS block as the input split for each mapper. It makes sure each mapper works on data on the same machine. 
  - If the task tracker could already been busy, in this case a different node would be chosen, and it would be streamed over the network.
  - Then the mapper passes the output to the reducer, then the reducer pass the final result back to HDFS. 


## MapReduce Design Pattern

### Overview of Patterns
- Filtering patterns
  - Sampling patterns
  - Top N

- Summerization patterns
  - Counting 
  - Min/Max
  - Men/median/mode
  - Index
  
- Structural patterns
  - Combining data set
  
### Filtering patterns
- Don't change the record in data. 
- Bloom filter: efficient probabilistic filter

### Summerization patterns
- Inverted Index. 
- Numerical summarization
  - min/max
  - count
  - mean/medium/model
  - first/last
  
  
# Big Data Specialization.

## Introduction to Big Data

### Big Data Generated By Machine: Advantages

*If you look at some of the sensors that contribute to the half terabyte of data generated on a plane, we will find that some of it comes from accelerometers that measure turbulence. There are also sensors built into the engines for temperature, pressure, many other measurable factors to detect engine malfunctions. Constant real-time analysis of all the data collected provides help monitoring and problem detection at 40,000 feet. That's approximately 12,000 meters above ground.*



### Big Data Generated By People: The Unstructured Challenge
Hadoop is designed to support the processing of large data sets in a distributed computing environment. Hadoop can handle big batches of distributed information but most often there's a need for a real time processing of people generated data like Twitter or Facebook updates.

Storm and Spark are two other open source frameworks that handle such real time data generated at a fast rate. Both Storm and Spark can integrate data with any database or data storage technology.

As we have emphasized before unstructured data does not have a relational data model so it doesn't generally fit into the traditional data warehouse model based on relational databases. Many businesses today are using a hybrid approach in which their smaller structured data remains in their relational databases, and large unstructured datasets get stored in NoSQL databases in the cloud.

NoSQL Data technologies are based on non-relational concepts and provide data storage options typically on computing clouds beyond the traditional relational databases centered rate houses. The main advantage of using NoSQL solutions is their ability to organize the data for scalable access to fit the problem and objectives pertaining to how the data will be used. For example, if the data will be used in an analysis to find connections between data sets, then the best solution is a graph database. Neo4j is an example of a graph database. If the data will be best accessed using key value pairs like a search engine scenario, the best solution is probably a dedicated key value paired database. Cassandra is an example of a key value database.

*Sentiment analysis analyzes social media and other data to find whether people associate positively or negatively with you business. Organizations are utilizing processing of personal data to understand the true preferences of their customers.*

*Another example application area for people generated data is customer behavior modeling and prediction. Amazon, Netflix and a lot of other organizations, use analytics to analyze preferences of their customers. Based on consumer behavior, organizations suggest better products to customers, and in turn have happier customers and higher profits.*
