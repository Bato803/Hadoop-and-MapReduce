# Hapdoop and MapReduce Notes

## Core Hadoop
1. Store data in HDFS (Hapdoop distributed file system), and process data in MapReduce. 
2. Key of the Core Hadoop system work: 
  - We split the data, and store it across a collection of machines, known as a cluster. And we process it where it was stored, rather than retrieving data from central server. So it's processed in place. 
  - More machine can be added to the cluster as the amount of data grows. 
3. Hadoop ecosystem:
  - Hive(turns SQL into MapReduce code, and then run it in the cluster), Pig; (These two might need more time because it's still running MapReduce code.)
  - Impala: Write SQL to directly access HDFS, it's optimized for low latency query. (No need to translated to MapReduce.) Hive is optimzied for long, batched processing job. 
  - Sqoop: Takes data from traditional relational database, and put it in HDFS files. 
  - Flume: Put external generated data into HDFS. 
  - HBase/Hue/Oozie/mahout....
  - CDH: Package all of them...Far easier to install CDH to have everything. 
  

## Hadoop Distributed File System
1. One file is split into different blocks, such as blk_1, blk_2, blk_3. 
2. Each block is store in one node of the cluster, separately. Namenode store the metadata about which block is stored in which node. 
3. To make the system more safe, Hadoop doesn't just store the block in one node. But in three nodes instead. This takes care of the problem that if one of the data nodes fail. 
4. To solve the problem if the meta node fails, people doesn't just store the namenode in its own harddrive, but also somewhere in the network system (NFS); There is an alternative solution as well, active namenode and standby namenode. 
5. Mapper and Reducer. 
  - Each mapper deals with a small amount of data and works in parallel, the output is called intermedimate records. (The record is dealt with in the form of (key, value))
  - Once the mappers have finished, a phase called shuffle and sort take place. Shuffle: the movement of intermediate record from mappers to reducers; Sort: the reducer would organize the records. 
6. When we run a map reduce job, we submit the job to what's called job tracker. That splits the job into mappers and reducers. 
  - There is a task tracker in each of the data node. The task tracker and the data node run at the same machine, the Hadoop framework would be able to have the map task tracker work directly on the pieces of data that are stored on the machine. That save a lot of traffics. 
  - Each mapper processes a portion of the input data, that's known as the input split. And by default, Hadoop would use a HDFS block as the input split for each mapper. It makes sure each mapper works on data on the same machine. 
  - If the task tracker could already been busy, in this case a different node would be chosen, and it would be streamed over the network.
  - Then the mapper passes the output to the reducer, then the reducer pass the final result back to HDFS. 


## MapReduce Design Pattern

### Overview of Patterns
- Filtering patterns
  - Sampling patterns
  - Top N

- Summerization patterns
  - Counting 
  - Min/Max
  - Men/median/mode
  - Index
  
- Structural patterns
  - Combining data set
  
### Filtering patterns
- Don't change the record in data. 
- Bloom filter: efficient probabilistic filter

### Summerization patterns
- Inverted Index. 
- Numerical summarization
  - min/max
  - count
  - mean/medium/model
  - first/last
  
  
# Big Data Specialization.

## Introduction to Big Data

### Big Data: Why and Where
#### Big Data Generated By Machine: Advantages

*If you look at some of the sensors that contribute to the half terabyte of data generated on a plane, we will find that some of it comes from accelerometers that measure turbulence. There are also sensors built into the engines for temperature, pressure, many other measurable factors to detect engine malfunctions. Constant real-time analysis of all the data collected provides help monitoring and problem detection at 40,000 feet. That's approximately 12,000 meters above ground.*

Previously, in traditional relational database management systems, data was often moved to computational space for processing. In Big Data space In-Situ means bringing the computation to where data is located or, in this case, generated. A key feature of these types of real-time notifications is that they enable real-time actions. However, using such a capability would require you to approach your application and your work differently.

Most Big Data centric businesses have updated their culture to be more real-time action oriented, refining real-time processes to handle anything from customer relations and fraud detection, to system monitoring and control.

SCADA is a type of industrial control system for remote monitoring and control of industrial processes that exists in the physical world, potentially including multiple sites, many types of sensors.

*In addition to monitoring and control, SCADA system can be used to define actions for reduced waste and improved efficiency in industrial processes, including those of manufacturing and power generation, public or private infrastructure processes, including water treatment, oil, and gas pipelines, and electrical power transmission, and facility processes including buildings, airports, ships, and space stations. They can even be used in smart building applications to monitor and control heating, ventilation, air conditioning systems like HVAC, access, and energy consumption.*

Again, the management of these processes once the trends, patterns, and anomalies are identified in real-time needs to be decided in the Big Data case. As a summary, as the largest and fastest type of Big Data, machine generated data can uniquely enable real-time actions in many systems and processes.

#### Big Data Generated By People: The Unstructured Challenge
Hadoop is designed to support the processing of large data sets in a distributed computing environment. Hadoop can handle big batches of distributed information but most often there's a need for a real time processing of people generated data like Twitter or Facebook updates.

Storm and Spark are two other open source frameworks that handle such real time data generated at a fast rate. Both Storm and Spark can integrate data with any database or data storage technology.



As we have emphasized before unstructured data does not have a relational data model so it doesn't generally fit into the traditional data warehouse model based on relational databases. Many businesses today are using a hybrid approach in which their smaller structured data remains in their relational databases, and large unstructured datasets get stored in NoSQL databases in the cloud.

NoSQL Data technologies are based on non-relational concepts and provide data storage options typically on computing clouds beyond the traditional relational databases centered rate houses. The main advantage of using NoSQL solutions is their ability to organize the data for scalable access to fit the problem and objectives pertaining to how the data will be used. For example, if the data will be used in an analysis to find connections between data sets, then the best solution is a graph database. Neo4j is an example of a graph database. If the data will be best accessed using key value pairs like a search engine scenario, the best solution is probably a dedicated key value paired database. Cassandra is an example of a key value database.

*Sentiment analysis analyzes social media and other data to find whether people associate positively or negatively with you business. Organizations are utilizing processing of personal data to understand the true preferences of their customers.*

*Another example application area for people generated data is customer behavior modeling and prediction. Amazon, Netflix and a lot of other organizations, use analytics to analyze preferences of their customers. Based on consumer behavior, organizations suggest better products to customers, and in turn have happier customers and higher profits.*

#### Organization-Generated Data: Structured by often siloed

Each organization has distinct operation practices and business models, which result in a variety of data generation platforms. For example, the type and source of data that a bank gets is very different from what a hardware equipment manufacturer gets. Some common types of organizational big data come from commercial transactions, credit cards, government institutions, e-commerce, banking or stock records, medical records, sensors, transactions, clicks and so on.

*Let's say you're an organization that collects sales transactions. You can use this data for pattern recognition to detect correlated products, to estimate demand for products likely to go up in sales, and capture fraudulent activity. Moreover, when you know your sales record and can correlate it with your marketing records, you can find which campaigns really made an impact. You are already becoming a data savvy organization. Now think of bringing your sales data together with other external open public data, such as major world events in the news. You can ask, was it savvy marketing or a consequence of external events that triggered your sales? Using proper analytics, you can now build inventories to match your predicted growth and demand. In addition, organizations build and apply processes to record and monitor business events of interest, such as registering a customer, manufacturing a product or taking an order. These processes collect highly structured data that include transactions, reference tables, and relationships, as well as the metadata that sets its context.*

Many organizations have traditionally captured data at the department level, without proper infrastructure and policy to share and integrate this data. This has hindered the growth of scalable pattern recognition to the benefits of the entire organization. Because no one system has access to all data that the organization owns. Each data set is compartmentalized. If such silos are left untouched, organizations risk having outdated, unsynchronized, and even invisible data sets.

As a summary, while highly structured organizational data is very useful and trustworthy, and thus a valuable source of information, organizations must pay special attention to breaking up the silos of information to make full use of its potential.

#### Organization-Generation Data: Benefits come from combining with other data types. 

*UPS delivers 16 million shipments per day. They get around 40 million tracking requests. That's huge. Can you guess how much money UPS can save by reducing each driver's route by just one mile? If they can reduce distance traveled by each truck by even one mile, UPS can save a whopping $50 million U.S. per year.* 


* An organization from the retail shopping domain that heavily utilizes big data is Walmart. Walmart is a big organization that gets 250 million customers in 10,000 stores. Did you know they collect 2.5 petabytes of data per hour? They collect data on Twitter tweets, local events, local weather, in-store purchases, online clicks and many other sales, customer and product related data. They use this data to find patterns such as which products are frequently purchased together, and what is the best new product to introduce in their stores, to predict demand at the particular location, and to customize customer recommendations.  *


### Characteristics of Big Data

#### Building a Big Data Strategy

As a summary, when building a big data strategy, it is important to integrate big data analytics with business objectives. Communicate goals and provide organizational buy-in for analytics projects. Build teams with diverse talents, and establish a teamwork mindset. Remove barriers to data access and integration. Finally, these activities need to be iterated to respond to new business goals and technological advances.

#### How does data science heppen? Five components of data science. 

Here we define data science as a multi-disciplinary craft that combines people teaming up around application-specific purpose that can be achieved through a process, big data computing platforms, and programmability.

Five steps: Acquire, prepare, analyze, report, act. 

#### Step One: Acquiring Data
- Relational database -- SQL
- Text or Excel spreadsheets -- scripting languages (Javascript, python, PHP, Perl, R, Matlab)
- Website in XML-- REST(Representational State Transfer), Web socket (real time), ..
- NoSQL (Cassandra, MongoDB, HBASE) -- API provided, web service also provided. 

#### Step Two: Exploring Data

In this step, you'll be looking for things like correlations, general trends, and outliers.

Correlation graphs can be used to explore the dependencies between different variables in the data. Graphing the general trends of variables will show you if there is a consistent direction in which the values of these variables are moving towards

Additionally, summary statistics provide numerical values to describe your data. 

#### Step Three: Pre-Processing Data

There are two main goals in the data pre-processing step. The first is to clean the data to address data quality issues, and the second is to transform the raw data to make it suitable for analysis.

**Data Quality**: Inconsistent data, duplicate customer records, missing value and invalid data. This domain knowledge is essential to making informed decisions on how to handle incomplete or incorrect data.

Here are some approaches we can take to address this quality issues:\
1. We can remove data records with missing values.
2. We can merge duplicate records. This will require a way to determine how to resolve conflicting values.Perhaps it makes sense to retain the newer value whenever there's a conflict.
3. For invalid values, the best estimate for a reasonable value can be used as a replacement. For example, for a missing age value for an employee, a reasonable value can be estimated based on the employee's length of employment.

**The second part of preparing data is to manipulate the clean data into the format needed for analysis.**

It includes scaling, transformation, feature selection, dimensionality reduction, and data manipulation. 

1. Scaling involves changing the range of values to be between a specified range. Such as from zero to one.
2. Various transformations can be performed on the data to reduce noise and variability. Aggregate data generally results in data with less variability, which may help with your analysis.
3. Future selection can involve removing redundant or irrelevant features, combining features, and creating new features. For example, the purchase price of a product and the amount of sales tax paid, are likely to be correlated.
4. Dimensionality reduction is useful when the data set has a large number of dimensions. It involves finding a smaller subset of dimensions that captures most of the variation in the data.

In summary, data preparation is a very important part of the data science process. In fact, this is where you will spend most of your time on any data science effort.

It can be a tedious process, but it is a crucial step. Always remember, garbage in, garbage out. If you don't spend the time and effort to create good data for the analysis, you will not get good results no matter how sophisticated the analysis technique you're using is.

### Basic Scalable Computing Concepts. 

#### Scalable Computing over the Internet

Simply put, a parallel computer is a very large number of single computing nodes with specialized capabilities connected to other network. For example, the Gordon Supercomputer here at The San Diego Supercomputer Center, has 1,024 compute nodes with 16 cores each equalling 16,384 compute cores in total.

**Commodity clusters** are affordable parallel computers with an average number of computing nodes. They are not as powerful as traditional parallel computers and are often built out of less specialized nodes.

In commodity clusters, the computing nodes are clustered in **racks** connected to each other via a fast network. Computing in one or more of these clusters across a local area network or the internet is called **distributed computing**. Such architectures enable what we call data-parallelism. In data-parallelism many jobs that share nothing can work on different data sets or parts of a data set.


A node, or an entire rack can fail at any given time. The connectivity of a rack to the network can stop or the connections between individual nodes can break. It is not practical to restart everything every time, if failure happens. The ability to recover from such failures is called **Fault-tolerance**.

**As a summary** the commodity clusters are a cost effective way of achieving data parallel scalability for big data applications. These type of systems have a higher potential for partial failures. It is this type of distributed computing that pushed for a change towards cost effective reliable and Fault-tolerant systems for management and analysis of big data.

#### Programming model for Big Data

MapReduce is a big data programming model that supports all the requirements of big data modeling we mentioned. It can model processing large data, split complications into different parallel tasks and make efficient use of large commodity clusters and distributed file systems. In addition, it abstracts out the details of parallelzation, full tolerance, data distribution, monitoring and load balancing.


### Getting Started with Hadoop

***What's in Hadoop ecosystem?***

1. They provide scalability to store large volume of data on commodity hardware. 
2. Supported graceful recovery from crashes and hardware failures. 
3. Handle different types of data. 
4. Provided the ability to facilitate a shared environment. (It's important to allow multiple jobs to exeucate simultaneously)
5. Open-source projects backed by a large community. 

Three main part of Hadoop:
1. MapReduce: a programming model for processing big data. 
2. YARN: the scheduler and resource manager. 
3. HDFS: The Hadoop distributed file system. 

***Layers in the Ecosystem***

Low level: storage and scheduling. 
High leve: Interactivity. 

Layer 0: HDFS, the lowest level, it provides scalable storage and fault tolerance. 
Layer 1: YARN, provide flexible scheduling and resource management over the HDFS storage. 
Layer 2: MapReduce, a programming model that simplifies parallel computing. 
Layer 3: Hive (dataflow scripting), Pig(SQL-like queries). It augments data modeling of MapReduce. 

***Layer 0: HDFS***
Key features: Scalability to large datasets, reliability to cope with hardware failures.

1. It achieves scalability by partitioning or splitting large files across multiple computers. 
2. By default, HDFS maintains three copies of every block (of file). 
3. HDFS is also designed to handle a variety of data types.

Key components: Namenode for metadata, DataNode for block storage. Usually one NameNode per cluster, one DataNode runs on each node in the cluster. 

NameNode: Coordinates operations. It keeps track of file name, location in directory, etc. It also record mapping of contents on DataNode (which files are stored on which nodes.). 

DataNode: Store the file blocks. It listens to commands from the name node for block creation, deletion, replication. 

***Layer 1: YARN***
It's a resource manager that sits just above the storage layer HDFS. 

Adding YARN in between HDFS and the applications enabled new systems to be built, focusing on different types of big data applications such as Giraph for graph data analysis, Storm for streaming data analysis, and Spark for in-memory analysis.

YARN does so by providing a standard framework that supports customized application development in the HADOOP ecosystem. 

Essential gears in the YARN engine:
1. Resource Manager. The resource manager controls all the resources, and decides who gets what.
2. Application Master. It negotiates resource from the Resource Manager and it talks to Node Manager to get its tasks completed.
3. Node Manager. Node manager operates at machine level and is in charge of a single machine.
4. Container. 

***Layer 2: MapReduce***

MapReduce is a simple programming model for the Hadoop ecosystem. It relies on YARN to schedule and execuate parallel processing over the distributed file blocks in HDFS. 

Traditional parallel programming requires expertise on a number of computing and system concepts. The MapReduce programming model greatly simplified running code in parallel. 

1. Map = apply operation to all elements. 
 - The first step in MapReduce is to run a map operation on each node. Note that map goes to each node containing a data block for the file, instead of the data moving to map. This is moving computation to data.It generates key-value pair in this step. 
 - The second step: sort and shuffle. The key values, with the same word, are moved to the same node. In general, a node will have different words. 
2. Reduce = summarize operation on elements. The reduce operation operate on the output from step 2, sort and shuffle, 
and executes on these nodes to add values for key-value pairs with the same keys

While MapReduce excels at independent batch tasks similar to our applications, there are certain kinds of tasks that you would not want to use MapReduce for.

1. For example, if your data is frequently changing, MapReduce is slow since it reads the entire input data set each time.
2. The MapReduce model requires that maps and reduces execute independently of each other. This greatly simplifies your job as a designer, since you do not have to deal with synchronization issues. However, it means that computations that do have dependencies, cannot be expressed with MapReduce.
3. Finally, MapReduce does not return any results until the entire process is finished. It must read the entire input data set. This makes it unsuitable for interactive applications where the results must be presented to the user very quickly, expecting a return from the user.

#### When to use Hadoop.

When to use:
1. Future anticipated data growth. 
2. Long term availability of data. 
3. Use multiple applications over the same data source. 
4. High volume. 
5. High variety. 

When not to use:
1. Small datasets. 
2. Task level parallelism. Hadoop is good for data parallelism, which means execution of the same function on multiple modes across the elements of a dataset. But it's not good for simultaneous execution of many different functions on multiple nodes. 
3. Advanced algorithms. 
4. Replacement to infrastructure. 
5. HDFS stores data in blocks of 64MB or larger, so you may have to read an entire file just to pick one data entry. So random access to data is difficult. 


#### Cloud Computing: An important big data enabler
Cloud = IT infrastructure & Applications on Rent over the Internet. 

If we build in-house capabilities, we need to buy hardware that suits our requirements. These includes, but not limited to, networking hardware, storage disks, upgrading hardware when it becomes obsolete. 

Similarily, software stacks are also complex. 

As for cloud, you pay as you go. 
1. That means a low capital investment. 
2. Deploying your application on a server that is geographically closer to your client can give you fast service and happy customers.
3. You don't need to have a five or ten year resource estimation plan.

In summary, PROS:
1. Focus on domain expertise for team members. 
2. Excellent value proposition. 
3. Scalability. 
4. Resource management. 

