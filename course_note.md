# Hapdoop and MapReduce Notes

## Core Hadoop
1. Store data in HDFS (Hapdoop distributed file system), and process data in MapReduce. 
2. Key of the Core Hadoop system work: 
  - We split the data, and store it across a collection of machines, known as a cluster. And we process it where it was stored, rather than retrieving data from central server. So it's processed in place. 
  - More machine can be added to the cluster as the amount of data grows. 
3. Hadoop ecosystem:
  - Hive(turns SQL into MapReduce code, and then run it in the cluster), Pig; (These two might need more time because it's still running MapReduce code.)
  - Impala: Write SQL to directly access HDFS, it's optimized for low latency query. (No need to translated to MapReduce.) Hive is optimzied for long, batched processing job. 
  - Sqoop: Takes data from traditional relational database, and put it in HDFS files. 
  - Flume: Put external generated data into HDFS. 
  - HBase/Hue/Oozie/mahout....
  - CDH: Package all of them...Far easier to install CDH to have everything. 
  

## Hadoop Distributed File System
1. One file is split into different blocks, such as blk_1, blk_2, blk_3. 
2. Each block is store in one node of the cluster, separately. Namenode store the metadata about which block is stored in which node. 
3. To make the system more safe, Hadoop doesn't just store the block in one node. But in three nodes instead. This takes care of the problem that if one of the data nodes fail. 
4. To solve the problem if the meta node fails, people doesn't just store the namenode in its own harddrive, but also somewhere in the network system (NFS); There is an alternative solution as well, active namenode and standby namenode. 
5. Mapper and Reducer. 
  - Each mapper deals with a small amount of data and works in parallel, the output is called intermedimate records. (The record is dealt with in the form of (key, value))
  - Once the mappers have finished, a phase called shuffle and sort take place. Shuffle: the movement of intermediate record from mappers to reducers; Sort: the reducer would organize the records. 
6. When we run a map reduce job, we submit the job to what's called job tracker. That splits the job into mappers and reducers. 
  - There is a task tracker in each of the data node. The task tracker and the data node run at the same machine, the Hadoop framework would be able to have the map task tracker work directly on the pieces of data that are stored on the machine. That save a lot of traffics. 
  - Each mapper processes a portion of the input data, that's known as the input split. And by default, Hadoop would use a HDFS block as the input split for each mapper. It makes sure each mapper works on data on the same machine. 
  - If the task tracker could already been busy, in this case a different node would be chosen, and it would be streamed over the network.
  - Then the mapper passes the output to the reducer, then the reducer pass the final result back to HDFS. 


## MapReduce Design Pattern

### Overview of Patterns
- Filtering patterns
  - Sampling patterns
  - Top N

- Summerization patterns
  - Counting 
  - Min/Max
  - Men/median/mode
  - Index
  
- Structural patterns
  - Combining data set
  
### Filtering patterns
- Don't change the record in data. 
- Bloom filter: efficient probabilistic filter

### Summerization patterns
- Inverted Index. 
- Numerical summarization
  - min/max
  - count
  - mean/medium/model
  - first/last
  
  
# Big Data Specialization.

## Introduction to Big Data

### Big Data: Why and Where
#### Big Data Generated By Machine: Advantages

*If you look at some of the sensors that contribute to the half terabyte of data generated on a plane, we will find that some of it comes from accelerometers that measure turbulence. There are also sensors built into the engines for temperature, pressure, many other measurable factors to detect engine malfunctions. Constant real-time analysis of all the data collected provides help monitoring and problem detection at 40,000 feet. That's approximately 12,000 meters above ground.*

Previously, in traditional relational database management systems, data was often moved to computational space for processing. In Big Data space In-Situ means bringing the computation to where data is located or, in this case, generated. A key feature of these types of real-time notifications is that they enable real-time actions. However, using such a capability would require you to approach your application and your work differently.

Most Big Data centric businesses have updated their culture to be more real-time action oriented, refining real-time processes to handle anything from customer relations and fraud detection, to system monitoring and control.

SCADA is a type of industrial control system for remote monitoring and control of industrial processes that exists in the physical world, potentially including multiple sites, many types of sensors.

*In addition to monitoring and control, SCADA system can be used to define actions for reduced waste and improved efficiency in industrial processes, including those of manufacturing and power generation, public or private infrastructure processes, including water treatment, oil, and gas pipelines, and electrical power transmission, and facility processes including buildings, airports, ships, and space stations. They can even be used in smart building applications to monitor and control heating, ventilation, air conditioning systems like HVAC, access, and energy consumption.*

Again, the management of these processes once the trends, patterns, and anomalies are identified in real-time needs to be decided in the Big Data case. As a summary, as the largest and fastest type of Big Data, machine generated data can uniquely enable real-time actions in many systems and processes.

#### Big Data Generated By People: The Unstructured Challenge
Hadoop is designed to support the processing of large data sets in a distributed computing environment. Hadoop can handle big batches of distributed information but most often there's a need for a real time processing of people generated data like Twitter or Facebook updates.

Storm and Spark are two other open source frameworks that handle such real time data generated at a fast rate. Both Storm and Spark can integrate data with any database or data storage technology.



As we have emphasized before unstructured data does not have a relational data model so it doesn't generally fit into the traditional data warehouse model based on relational databases. Many businesses today are using a hybrid approach in which their smaller structured data remains in their relational databases, and large unstructured datasets get stored in NoSQL databases in the cloud.

NoSQL Data technologies are based on non-relational concepts and provide data storage options typically on computing clouds beyond the traditional relational databases centered rate houses. The main advantage of using NoSQL solutions is their ability to organize the data for scalable access to fit the problem and objectives pertaining to how the data will be used. For example, if the data will be used in an analysis to find connections between data sets, then the best solution is a graph database. Neo4j is an example of a graph database. If the data will be best accessed using key value pairs like a search engine scenario, the best solution is probably a dedicated key value paired database. Cassandra is an example of a key value database.

*Sentiment analysis analyzes social media and other data to find whether people associate positively or negatively with you business. Organizations are utilizing processing of personal data to understand the true preferences of their customers.*

*Another example application area for people generated data is customer behavior modeling and prediction. Amazon, Netflix and a lot of other organizations, use analytics to analyze preferences of their customers. Based on consumer behavior, organizations suggest better products to customers, and in turn have happier customers and higher profits.*

#### Organization-Generated Data: Structured by often siloed

Each organization has distinct operation practices and business models, which result in a variety of data generation platforms. For example, the type and source of data that a bank gets is very different from what a hardware equipment manufacturer gets. Some common types of organizational big data come from commercial transactions, credit cards, government institutions, e-commerce, banking or stock records, medical records, sensors, transactions, clicks and so on.

*Let's say you're an organization that collects sales transactions. You can use this data for pattern recognition to detect correlated products, to estimate demand for products likely to go up in sales, and capture fraudulent activity. Moreover, when you know your sales record and can correlate it with your marketing records, you can find which campaigns really made an impact. You are already becoming a data savvy organization. Now think of bringing your sales data together with other external open public data, such as major world events in the news. You can ask, was it savvy marketing or a consequence of external events that triggered your sales? Using proper analytics, you can now build inventories to match your predicted growth and demand. In addition, organizations build and apply processes to record and monitor business events of interest, such as registering a customer, manufacturing a product or taking an order. These processes collect highly structured data that include transactions, reference tables, and relationships, as well as the metadata that sets its context.*

Many organizations have traditionally captured data at the department level, without proper infrastructure and policy to share and integrate this data. This has hindered the growth of scalable pattern recognition to the benefits of the entire organization. Because no one system has access to all data that the organization owns. Each data set is compartmentalized. If such silos are left untouched, organizations risk having outdated, unsynchronized, and even invisible data sets.

As a summary, while highly structured organizational data is very useful and trustworthy, and thus a valuable source of information, organizations must pay special attention to breaking up the silos of information to make full use of its potential.

#### Organization-Generation Data: Benefits come from combining with other data types. 

*UPS delivers 16 million shipments per day. They get around 40 million tracking requests. That's huge. Can you guess how much money UPS can save by reducing each driver's route by just one mile? If they can reduce distance traveled by each truck by even one mile, UPS can save a whopping $50 million U.S. per year.* 


* An organization from the retail shopping domain that heavily utilizes big data is Walmart. Walmart is a big organization that gets 250 million customers in 10,000 stores. Did you know they collect 2.5 petabytes of data per hour? They collect data on Twitter tweets, local events, local weather, in-store purchases, online clicks and many other sales, customer and product related data. They use this data to find patterns such as which products are frequently purchased together, and what is the best new product to introduce in their stores, to predict demand at the particular location, and to customize customer recommendations.  *


### Characteristics of Big Data

#### Building a Big Data Strategy

As a summary, when building a big data strategy, it is important to integrate big data analytics with business objectives. Communicate goals and provide organizational buy-in for analytics projects. Build teams with diverse talents, and establish a teamwork mindset. Remove barriers to data access and integration. Finally, these activities need to be iterated to respond to new business goals and technological advances.

#### How does data science heppen? Five components of data science. 

Here we define data science as a multi-disciplinary craft that combines people teaming up around application-specific purpose that can be achieved through a process, big data computing platforms, and programmability.

Five steps: Acquire, prepare, analyze, report, act. 

#### Step One: Acquiring Data
- Relational database -- SQL
- Text or Excel spreadsheets -- scripting languages (Javascript, python, PHP, Perl, R, Matlab)
- Website in XML-- REST(Representational State Transfer), Web socket (real time), ..
- NoSQL (Cassandra, MongoDB, HBASE) -- API provided, web service also provided. 

#### Step Two: Exploring Data

In this step, you'll be looking for things like correlations, general trends, and outliers.

Correlation graphs can be used to explore the dependencies between different variables in the data. Graphing the general trends of variables will show you if there is a consistent direction in which the values of these variables are moving towards

Additionally, summary statistics provide numerical values to describe your data. 

#### Step Three: Pre-Processing Data

There are two main goals in the data pre-processing step. The first is to clean the data to address data quality issues, and the second is to transform the raw data to make it suitable for analysis.

**Data Quality**: Inconsistent data, duplicate customer records, missing value and invalid data. This domain knowledge is essential to making informed decisions on how to handle incomplete or incorrect data.

Here are some approaches we can take to address this quality issues:\
1. We can remove data records with missing values.
2. We can merge duplicate records. This will require a way to determine how to resolve conflicting values.Perhaps it makes sense to retain the newer value whenever there's a conflict.
3. For invalid values, the best estimate for a reasonable value can be used as a replacement. For example, for a missing age value for an employee, a reasonable value can be estimated based on the employee's length of employment.

**The second part of preparing data is to manipulate the clean data into the format needed for analysis.**

It includes scaling, transformation, feature selection, dimensionality reduction, and data manipulation. 

1. Scaling involves changing the range of values to be between a specified range. Such as from zero to one.
2. Various transformations can be performed on the data to reduce noise and variability. Aggregate data generally results in data with less variability, which may help with your analysis.
3. Future selection can involve removing redundant or irrelevant features, combining features, and creating new features. For example, the purchase price of a product and the amount of sales tax paid, are likely to be correlated.
4. Dimensionality reduction is useful when the data set has a large number of dimensions. It involves finding a smaller subset of dimensions that captures most of the variation in the data.

In summary, data preparation is a very important part of the data science process. In fact, this is where you will spend most of your time on any data science effort.

It can be a tedious process, but it is a crucial step. Always remember, garbage in, garbage out. If you don't spend the time and effort to create good data for the analysis, you will not get good results no matter how sophisticated the analysis technique you're using is.
